# ML Сервис

Микросервис для обработки AI-запросов с использованием модели Qwen и message queue на базе Redis.

## Быстрый старт

1. Убедитесь, что у вас установлен Docker и Docker Compose
2. Клонируйте репозиторий
3. Перейдите в директорию ml_service
4. Запустите сервис:
```bash
docker-compose up --build
```

**ВАЖНО** После этого сервис через docker network подтянется к редису, который мы подняли в ./backend. В этом случае redis выступает как message queue.


## Обзор

ML Сервис разработан для асинхронной обработки AI-запросов через очередь Redis. Он обрабатывает текстовые входные данные и возвращает ответы, сгенерированные искусственным интеллектом, что делает его подходящим для интеграции с более крупными приложениями, требующими возможностей AI.

## Возможности

- Асинхронная обработка запросов
- Система очередей на базе Redis
- Контейнеризация с помощью Docker
- Надёжная обработка ошибок и логирование
- Поддержка обработки текстовых данных
- Сохранение моделей с помощью монтирования томов

## Требования

- Docker и Docker Compose
- Python 3.8 или выше (для локальной разработки)
- Redis сервер (обеспечивается Docker Compose)

## Установка

### Локальная разработка

1. Создайте виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate  # Для Windows: .\venv\Scripts\activate
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Структура проекта

```
ml_service/
├── app/
│   ├── main.py          # Точка входа сервиса
│   ├── agent.py         # Логика обработки AI
│   └── llm.py           # Интерфейс языковой модели
├── models/              # Директория для хранения моделей
├── Dockerfile          # Конфигурация Docker
├── docker-compose.yml  # Конфигурация Docker Compose
├── requirements.txt    # Python зависимости
└── README.md          # Этот файл
```

## Конфигурация

Сервис может быть настроен через переменные окружения:

- `REDIS_URL`: URL подключения к Redis (по умолчанию: "redis://redis_db:6379")

## Архитектура

Сервис использует архитектуру на основе очередей:

1. Запросы помещаются в очередь Redis (`ai:processing:queue`)
2. Сервис асинхронно обрабатывает запросы
3. Результаты сохраняются в Redis с ключами в формате `ai:complete:{request_uuid}`
4. Основной сервис поддерживает постоянное соединение с Redis

## Разработка

Для запуска сервиса локально в режиме разработки:

```bash
python -m app.main
```

## Лицензия

[Добавьте информацию о лицензии]

## Участие в разработке

[Добавьте руководство по участию в разработке]
